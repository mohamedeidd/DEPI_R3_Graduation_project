# DEPI_R3_Graduation_project
## **ğŸ“Œ Project Overview**
**ğŸ‘• Multi-Modal Clothing Classifier**

Fashion styles can be subjective, but by combining visual features from clothing images with textual features like product descriptions, we can build a multi-modal model that classifies clothing styles more accurately than single-input models.

# **ğŸ¯ Objectives**

- ğŸ–¼ï¸ Use CNNs for image features.

- ğŸ“ Use Transformers for text features.

- ğŸ”— Combine both to create a multi-modal classifier.

- ğŸ“ˆ Evaluate model performance on fashion datasets.

# **ğŸ› ï¸ Tech Stack**

**Language**: Python

**Deep Learning**: TensorFlow 

**Image Processing** : OpenCV, PIL

**Text Processing** : NLTK, HuggingFace Transformers

**Visualization** : Matplotlib, Seaborn

**Data Handling** : NumPy, Pandas

**Machine Learning** : sci-kit learn

ğŸ“‚ Project Structure
â”œâ”€â”€ data/                # Dataset (images + text)
â”œâ”€â”€ notebooks/           # Jupyter notebooks for experiments
â”œâ”€â”€ models/              # Saved trained models
â””â”€â”€ README.md            # Project documentation

# **ğŸ“Š Expected Results**

Better accuracy using multi-modal inputs rather than single-input models.

Visualizations like training curves, confusion matrix, and sample predictions.

# **ğŸ‘¥ Team Members**

Mohamed Eid

[Team Member 2 Name]

[Team Member 3 Name]

[Team Member 4 Name]

Instructor: *Aya Hesham*


# **âœ¨ Acknowledgments**

Special thanks to *Aya Hesham* for supervision and guidance throughout the whole learning process.
