# DEPI_R3_Graduation_project
ğŸ“Œ Project Overview

Fashion styles can be subjective, but by combining visual features from clothing images with textual features like product descriptions, we can build a multi-modal model that classifies clothing styles more accurately than single-input models.

ğŸ¯ Objectives

ğŸ–¼ï¸ Use CNNs for image features.

ğŸ“ Use Transformers/LSTMs for text features.

ğŸ”— Combine both to create a multi-modal classifier.

ğŸ“ˆ Evaluate model performance on fashion datasets.

ğŸ› ï¸ Tech Stack
Component	Technology
Language	Python
Frameworks	TensorFlow / PyTorch
Image Handling	OpenCV, PIL
Text Processing	NLTK, HuggingFace Transformers
Visualization	Matplotlib, Seaborn
Data Handling	NumPy, Pandas
ğŸ“‚ Project Structure
â”œâ”€â”€ data/                # Dataset files (images + text)
â”œâ”€â”€ notebooks/           # Jupyter notebooks for experiments
â”œâ”€â”€ src/                 # Source code for model, preprocessing, etc.
â”œâ”€â”€ models/              # Saved trained models
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation

ğŸš€ How to Run

Clone the repository:

git clone https://github.com/your-username/multi-modal-clothing-classifier.git
cd multi-modal-clothing-classifier


Install dependencies:

pip install -r requirements.txt


Run training:

python src/train.py


Evaluate the model:

python src/evaluate.py

ğŸ“Š Expected Results

ğŸ“ˆ Higher accuracy using multi-modal input vs single input.

ğŸ–¼ï¸ Visualizations: Training curves, confusion matrix, sample predictions.

Example Output:


ğŸ‘¥ Team Members

Mohamed Eid

[Team Member 2 Name]

[Team Member 3 Name]

[Team Member 4 Name]

Instructor: Aya Hesham

ğŸ“œ License

This project is licensed under the MIT License.

âœ¨ Acknowledgments

Special thanks to Aya Hesham for guidance and support throughout this project.
